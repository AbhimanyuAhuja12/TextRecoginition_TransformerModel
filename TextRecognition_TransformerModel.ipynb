{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb0c67f",
   "metadata": {},
   "source": [
    "# Making Transformer Model From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7d1ce",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebeb8a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2  # For PDF image extraction\n",
    "import docx  # For DOCX text extraction\n",
    "from PIL import Image  # For image loading\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # For Transformer layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e5883",
   "metadata": {},
   "source": [
    "# Extracting Images From Pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c523e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path):\n",
    "  \"\"\"\n",
    "  Extracts images from a PDF file using PyPDF2.\n",
    "\n",
    "  Args:\n",
    "      pdf_path: Path to the PDF file.\n",
    "\n",
    "  Returns:\n",
    "      A list of extracted image objects or None if no images found.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(pdf_path, 'rb') as pdf_file:\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    extracted_images = []\n",
    "\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "      page = pdf_reader.pages[page_num]\n",
    "\n",
    "      # Extract embedded images (adjust based on your PDF structure)\n",
    "      for image_data in page.extract_embedded_images():\n",
    "        # Assuming format and filename extraction logic (optional)\n",
    "        image_format = image_data['Format']\n",
    "        image_filename = f\"page_{page_num+1}_image_{len(extracted_images)}.{image_format}\"\n",
    "\n",
    "        # Save the image to a temporary location \n",
    "        with open(image_filename, 'wb') as image_file:\n",
    "          image_file.write(image_data['stream'])\n",
    "\n",
    "        extracted_images.append(image_filename)\n",
    "\n",
    "    return extracted_images if extracted_images else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ea7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_images_from_pdf('Padilla - Nobleza virtuosa_testExtract.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31717d85",
   "metadata": {},
   "source": [
    "# Expracting Text From Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be84399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_docx(docx_path):\n",
    "  \"\"\"\n",
    "  Extracts text from a DOCX file.\n",
    "\n",
    "  Args:\n",
    "      docx_path: Path to the DOCX file.\n",
    "\n",
    "  Returns:\n",
    "      The extracted text content as a string.\n",
    "  \"\"\"\n",
    "\n",
    "  doc = docx.Document(docx_path)\n",
    "  full_text = []\n",
    "\n",
    "  # Iterating through paragraphs in all document elements\n",
    "  for paragraph in doc.paragraphs:\n",
    "    full_text.append(paragraph.text)\n",
    "\n",
    "  return '\\n'.join(full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_from_docx('Padilla - 1 Nobleza virtuosa_testTranscription.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433f8f6",
   "metadata": {},
   "source": [
    "# Preprocessing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ebbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "  \"\"\"\n",
    "  Loads, resizes, and normalizes an image for the Transformer model.\n",
    "\n",
    "  Args:\n",
    "      image_path: Path to the image file.\n",
    "      target_size: Target size for resizing (default: (224, 224)).\n",
    "\n",
    "  Returns:\n",
    "      A NumPy array representing the preprocessed image.\n",
    "  \"\"\"\n",
    "\n",
    "  # Load the image using PIL\n",
    "  img = Image.open(image_path)\n",
    "\n",
    "  # Resize the image to the target size using high-quality Lanczos resampling\n",
    "  img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "  # Convert the image to a NumPy array and normalize to range [0, 1]\n",
    "  image_data = np.array(img) / 255.0\n",
    "\n",
    "  return image_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907aff70",
   "metadata": {},
   "source": [
    "# Transformer(Encoder-Decoder Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351d42e",
   "metadata": {},
   "source": [
    "Self-Attention Mechanism: This is the heart of the Transformer and allows each element in the sequence (words in the text or features from the image) to attend to other elements in the same sequence.\n",
    "\n",
    "Implementation Details:\n",
    "Each element is projected into three linear layers: query (Q), key (K), and value (V) vectors.\n",
    "\n",
    "The query vector of a specific element attends to the key vectors of all other elements.\n",
    "\n",
    "Attention weights are calculated based on the compatibility between the query and key vectors. These weights represent how relevant each element is to the element calculating the attention.\n",
    "\n",
    "The attention weights are used to weight the value vectors of all elements, creating a context vector for the specific element based on relevant information from other elements in the sequence.\n",
    "\n",
    "A multi-head self-attention mechanism performs multiple independent attention calculations with different weight projections, capturing diverse relationships within the sequence.\n",
    "\n",
    "The outputs from the multi-head attention are concatenated and projected through a linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad533a",
   "metadata": {},
   "source": [
    "***The layer takes the embedding dimension, number of heads, and optional dropout rate as hyperparameters.\n",
    "\n",
    "***It defines dense layers for projecting queries, keys, and values.\n",
    "\n",
    "***An additional dense layer combines the outputs from multiple heads.\n",
    "\n",
    "***A dropout layer is included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5e3d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  \"\"\"\n",
    "  Multi-head attention layer for the Transformer model.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, embedding_dim, num_heads, dropout_rate=0.1):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = embedding_dim // num_heads  # Dimension per head\n",
    "\n",
    "    # Dense layers for query, key, and value projections\n",
    "    self.dense_q = tf.keras.layers.Dense(embedding_dim)\n",
    "    self.dense_k = tf.keras.layers.Dense(embedding_dim)\n",
    "    self.dense_v = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    # Dense layer for combining multiple heads' outputs\n",
    "    self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    # Dropout layer\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    # Weights for scaling attention scores (optional)\n",
    "    self.scale = tf.Variable(1 / (self.head_dim ** 0.5), trainable=True, name='attention_scale')\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"\n",
    "    Splits the input tensor into multiple heads.\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])  # (batch_size, num_heads, seq_len, head_dim) \n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    \"\"\"\n",
    "    Performs multi-head attention on the input tensors.\n",
    "\n",
    "    Args:\n",
    "        inputs: A tuple of tensors (queries, keys, values).\n",
    "        training: Boolean, whether in training mode (affects dropout).\n",
    "\n",
    "    Returns:\n",
    "        The weighted sum of values based on attention scores.\n",
    "    \"\"\"\n",
    "\n",
    "    queries, keys, values = inputs\n",
    "    batch_size = tf.shape(queries)[0]\n",
    "\n",
    "    # Project queries, keys and values using dense layers\n",
    "    q = self.dense_q(queries)\n",
    "    k = self.dense_k(keys)\n",
    "    v = self.dense_v(values)\n",
    "\n",
    "    # Split heads for parallel attention calculations\n",
    "    q_heads = self.split_heads(q, batch_size)\n",
    "    k_heads = self.split_heads(k, batch_size)\n",
    "    v_heads = self.split_heads(v, batch_size)\n",
    "\n",
    "    # Calculate attention scores\n",
    "    attention_scores = tf.matmul(q_heads, k_heads, transpose_b=True) * self.scale\n",
    "\n",
    "    # Apply masking \n",
    "    # ... (implement masking logic based on your needs)\n",
    "\n",
    "    # Applying a softmax function to attention scores\n",
    "    attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "    # Apply dropout \n",
    "    attention_weights = self.dropout(attention_weights, training=training)\n",
    "\n",
    "    # Context vector weighted sum\n",
    "    context_vector = tf.matmul(attention_weights, v_heads)\n",
    "\n",
    "    # Combine heads back into a single tensor\n",
    "    context_vector = tf.transpose(context_vector, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, head_dim)\n",
    "    context_vector = tf.reshape(context_vector, (batch_size, -1, self.embedding_dim))\n",
    "\n",
    "    # Applying final linear layer\n",
    "    output = self.dense(context_vector)\n",
    "\n",
    "    return output\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(MultiHeadAttention, self).get_config()\n",
    "    config.update({\n",
    "        'embedding_dim': self.embedding_dim,\n",
    "        'num_heads': self.num_heads,\n",
    "        'dropout_rate': self.dropout_rate,\n",
    "    })\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ba17e",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Attention:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9f49a8",
   "metadata": {},
   "source": [
    "This attention mechanism allows the decoder to attend to the encoded representation of the image (generated by the encoder) while generating the caption.\n",
    "\n",
    "The decoder query vector attends to the encoder key and value vectors.\n",
    "\n",
    "Similar to self-attention, attention weights are calculated and used to create a context vector for the decoder, incorporating information from the encoded image representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc660d19",
   "metadata": {},
   "source": [
    "class TransformerDecoderLayer(keras.layers.Layer):\n",
    "  def __init__(self, embedding_dim, num_heads, dropout_rate):\n",
    "    super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "    self.self_attention = MultiHeadAttention(embedding_dim, num_heads)\n",
    "    self.encoder_decoder_attention = MultiHeadAttention(embedding_dim, num_heads)\n",
    "    # ... (other layer components)\n",
    "\n",
    "  def call(self, inputs, encoder_outputs, training=False):\n",
    "    # ... (decoder self-attention)\n",
    "\n",
    "    # Encoder-decoder attention\n",
    "    context_vector = self.encoder_decoder_attention((decoder_output, encoder_outputs, encoder_outputs), training=training)\n",
    "\n",
    "    # ... (add & norm, feed forward, add & norm)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab8fa7",
   "metadata": {},
   "source": [
    "# Evaluating Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24d04f",
   "metadata": {},
   "source": [
    "# BLEU SCORE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import BLEU\n",
    "\n",
    "# Load generated captions from a file\n",
    "generated_captions = []\n",
    "with open(\"generated_captions.txt\", \"r\") as file:\n",
    "  for line in file:\n",
    "    generated_captions.append(line.strip())\n",
    "\n",
    "# Load reference captions from a file \n",
    "reference_captions = []\n",
    "with open(\"reference_captions.txt\", \"r\") as file:\n",
    "  for line in file:\n",
    "    captions = file.readlines()  # Assuming multiple references per image\n",
    "    reference_captions.append(captions)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu = BLEU(score_type='corpus')\n",
    "bleu_score = bleu.corpus_score(generated_captions, reference_captions)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score.score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
